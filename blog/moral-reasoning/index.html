<!DOCTYPE html><html lang="en" class="dark" data-astro-cid-37fxchfa> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="description" content="How does chain-of-thought reasoning affect ethical decision making in LLMs?"><meta name="generator" content="Astro v5.16.9"><link rel="icon" type="image/png" href="/icon.png"><link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800&display=swap" rel="stylesheet"><title>Evaluating Long-Term Ethical Decision-Making Capabilities of Reasoning LLMs | Niall Dalton</title><meta property="og:title" content="Evaluating Long-Term Ethical Decision-Making Capabilities of Reasoning LLMs | Niall Dalton"><meta property="og:description" content="How does chain-of-thought reasoning affect ethical decision making in LLMs?"><meta property="og:type" content="website"><meta property="og:url" content="https://ndalton12.github.io/blog/moral-reasoning/"><!-- Prevent flash of wrong theme --><script>
      const theme = localStorage.getItem('theme') || 'dark';
      if (theme === 'light') {
        document.documentElement.classList.remove('dark');
      }
    </script><link rel="stylesheet" href="/_astro/_slug_.DXTyhdpv.css"></head> <body class="min-h-screen overflow-x-hidden bg-[#f8f9fa] text-gray-900 transition-colors duration-300 dark:bg-[#06060a] dark:text-gray-100" data-astro-cid-37fxchfa> <!-- Background --> <div class="fixed inset-0 -z-10 overflow-hidden" data-astro-cid-37fxchfa> <!-- Dot grid --> <div class="dot-grid" data-astro-cid-37fxchfa></div> <!-- Lower perspective grid --> <canvas id="grid-canvas" data-astro-cid-37fxchfa></canvas> <!-- Animated glow orbs --> <div class="glow glow-1" data-astro-cid-37fxchfa></div> <div class="glow glow-2" data-astro-cid-37fxchfa></div> <!-- Top radial glow --> <div class="absolute inset-0 bg-[radial-gradient(ellipse_60%_40%_at_50%_0%,rgba(99,102,241,0.08))] dark:bg-[radial-gradient(ellipse_60%_40%_at_50%_0%,rgba(120,119,198,0.12))]" data-astro-cid-37fxchfa></div> </div> <main class="relative" data-astro-cid-37fxchfa> <div class="mx-auto max-w-5xl px-6 py-12 sm:px-8 lg:px-12" data-astro-cid-37fxchfa>  <header class="mb-16 pt-8"> <div class="flex flex-col gap-8 md:flex-row md:items-start md:gap-12"> <!-- Profile Image --> <div class="flex-shrink-0"> <a href="/" class="block"> <div class="overflow-hidden rounded-2xl ring-1 ring-gray-200 dark:ring-white/10"> <img class="w-32 h-32 md:w-40 md:h-40 object-cover" src="/me3_compressed.jpeg" alt="Niall Dalton"> </div> </a> </div> <!-- Content --> <div class="flex-1 space-y-4"> <!-- Name with animated gradient --> <div class="flex items-start justify-between gap-4"> <a href="/" class="inline-block"> <h1 class="text-4xl md:text-5xl font-bold tracking-tight"> <span class="gradient-text">Niall Dalton</span> </h1> </a> <button id="theme-toggle" type="button" class="p-2 rounded-lg text-gray-400 hover:text-white hover:bg-white/5 transition-colors" aria-label="Toggle theme"> <!-- Sun icon (shown in dark mode) --> <svg class="w-5 h-5 hidden dark:block" fill="none" stroke="currentColor" viewBox="0 0 24 24"> <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364l-.707-.707M6.343 6.343l-.707-.707m12.728 0l-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z"></path> </svg> <!-- Moon icon (shown in light mode) --> <svg class="w-5 h-5 block dark:hidden" fill="none" stroke="currentColor" viewBox="0 0 24 24"> <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003 0 0012 21a9.003 9.003 0 008.354-5.646z"></path> </svg> </button> <script type="module">const a=document.getElementById("theme-toggle"),t=document.documentElement;function d(){const e=localStorage.getItem("theme");return e==="light"||e==="dark"?e:"dark"}function n(e){e==="dark"?t.classList.add("dark"):t.classList.remove("dark"),localStorage.setItem("theme",e),window.dispatchEvent(new CustomEvent("themechange",{detail:e}))}n(d());a?.addEventListener("click",()=>{const e=t.classList.contains("dark")?"dark":"light";n(e==="dark"?"light":"dark")});</script> </div> <!-- Tagline --> <p class="text-lg text-gray-500 dark:text-gray-400 font-light max-w-md"> I write code and think about stuff. </p> <!-- Navigation Links --> <nav class="pt-2"> <ul class="flex flex-wrap items-center gap-1"> <li> <a href="https://github.com/ndalton12" target="_blank" rel="noopener noreferrer" class="inline-flex items-center gap-2 px-4 py-2 text-sm font-medium text-gray-500 dark:text-gray-400 hover:text-gray-900 dark:hover:text-white rounded-lg transition-colors"> <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24"> <path fill-rule="evenodd" d="M12 2C6.477 2 2 6.484 2 12.017c0 4.425 2.865 8.18 6.839 9.504.5.092.682-.217.682-.483 0-.237-.008-.868-.013-1.703-2.782.605-3.369-1.343-3.369-1.343-.454-1.158-1.11-1.466-1.11-1.466-.908-.62.069-.608.069-.608 1.003.07 1.531 1.032 1.531 1.032.892 1.53 2.341 1.088 2.91.832.092-.647.35-1.088.636-1.338-2.22-.253-4.555-1.113-4.555-4.951 0-1.093.39-1.988 1.029-2.688-.103-.253-.446-1.272.098-2.65 0 0 .84-.27 2.75 1.026A9.564 9.564 0 0112 6.844c.85.004 1.705.115 2.504.337 1.909-1.296 2.747-1.027 2.747-1.027.546 1.379.202 2.398.1 2.651.64.7 1.028 1.595 1.028 2.688 0 3.848-2.339 4.695-4.566 4.943.359.309.678.92.678 1.855 0 1.338-.012 2.419-.012 2.747 0 .268.18.58.688.482A10.019 10.019 0 0022 12.017C22 6.484 17.522 2 12 2z" clip-rule="evenodd"></path> </svg>
GitHub
</a> </li> <li> <a href="https://www.linkedin.com/in/niall-dalton/" target="_blank" rel="noopener noreferrer" class="inline-flex items-center gap-2 px-4 py-2 text-sm font-medium text-gray-500 dark:text-gray-400 hover:text-gray-900 dark:hover:text-white rounded-lg transition-colors"> <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24"> <path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"></path> </svg>
LinkedIn
</a> </li> <li> <a href="/blog" class="inline-flex items-center gap-2 px-4 py-2 text-sm font-medium text-gray-500 dark:text-gray-400 hover:text-gray-900 dark:hover:text-white rounded-lg transition-colors"> <svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"> <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 20H5a2 2 0 01-2-2V6a2 2 0 012-2h10a2 2 0 012 2v1m2 13a2 2 0 01-2-2V7m2 13a2 2 0 002-2V9a2 2 0 00-2-2h-2m-4-3H9M7 16h6M7 8h6v4H7V8z"></path> </svg>
Blog
</a> </li> </ul> </nav> </div> </div> </header> <article> <header class="mb-12"> <div class="glass-card rounded-2xl p-6 md:p-8"> <a href="/blog" class="text-sm text-gray-500 hover:text-gray-900 dark:hover:text-white transition-colors">
← Back
</a> <h1 class="mt-6 text-3xl md:text-4xl font-bold text-gray-900 dark:text-white tracking-tight"> Evaluating Long-Term Ethical Decision-Making Capabilities of Reasoning LLMs </h1> <time class="mt-3 block text-sm text-gray-500"> May 4, 2025 </time> </div> </header> <div class="glass-card rounded-2xl p-6 md:p-8"> <div class="prose max-w-none">  <h1 id="are-reasoning-models-more-moral">Are Reasoning Models More Moral?</h1>
<p>Reasoning models like <a href="https://openai.com/index/introducing-openai-o1-preview/">o1</a> introduced the world to LLMs that
can “think” — that is, models which are trained to spell out their thoughts before answering, giving them time to brainstorm, plan,
and backtrack. Furthermore, it’s become clear that reasoning models are superior to their non-reasoning counterparts at
STEM problems. The latest in OpenAI’s o-series, o3, is capable of solving PhD level problems.</p>
<p>Despite their successes, reasoning models are not a silver bullet for everything. For example, they don’t necessarily
outperform non-reasoning models at <a href="https://eqbench.com/creative_writing_longform.html">longform writing</a>. Another area where
the benefits are unclear in ethical decision making, which, to my knowledge, has not yet been studied.</p>
<p>Note that I will use thinking and reasoning interchangeably throughout.</p>
<h1 id="setup-and-methodology">Setup and Methodology</h1>
<p>I hypothesized that reasoning models may perform substantially differently in complex, ethically challenging situations, perhaps
by preferring more game theoretic solutions to problems, instead of relying on more intrinsic heuristics (somewhat akin to reflexive action
in human decision making). I set out to study this by creating several morally/ethically ambiguous simulations and tasking LLMs with
participating in the simulations.</p>
<p>The research question can be stated as such: “Can current AI thinking models make consistently sound long-term decisions that align with human ethical principles when confronted with challenging, multi-stage simulated scenarios involving moral ambiguity and conflicting stakeholder interests?”</p>
<h2 id="measuring-outcomes">Measuring Outcomes</h2>
<p>I also assigned outcomes some utility value based on three moral frameworks, as noted below.</p>
<ul>
<li>Virtue theory: “Virtue Theory is an ethical framework that says that we ought to focus not on what rules to follow, but on what kinds of people (or organizations) we should be, and what kinds of ethical exemplars we ought to imitate” (<a href="https://conciseencyclopedia.org/entries/ethical-theory-virtue-theory/">https://conciseencyclopedia.org/entries/ethical-theory-virtue-theory/</a>)</li>
<li>Rawlsian minimax: “In ethics: Rawls’s theory of justice…is known as the “maximin” principle, because it seeks to maximize the welfare of those at the minimum level of society.” (<a href="https://www.britannica.com/topic/maximin-principle">https://www.britannica.com/topic/maximin-principle</a>)</li>
<li>Utilitarianism: “we should choose the option that “maximizes utility,” i.e. that action or policy that produces the largest amount of good.” (<a href="https://iep.utm.edu/util-a-r/">https://iep.utm.edu/util-a-r/</a>)</li>
</ul>
<p>The utility values are somewhat arbitrary, but represent (hopefully) good <em>relative</em> markers. I also recorded each decision made by an LLM
and re-ran each scenario 30 times.</p>
<h2 id="the-scenarios">The Scenarios</h2>
<p>Each scenario is represented by a branching decision tree, with some probability of transitioning to a node at a further level. I limited the depth of
the tree to 3 to avoid combinatorial explosion when creating the scenarios. I selected several criteria for making each scenario:</p>
<ul>
<li>Inherent ambiguity with no single “correct” solution;</li>
<li>The presence of multiple stakeholders possessing conflicting interests</li>
<li>Decision points with significant, cascading long-term consequences</li>
<li>The potential for well-intentioned actions to yield unintended negative outcomes</li>
<li>The necessity for trade-offs between competing ethical values (such as efficiency versus fairness or individual rights versus collective good).</li>
</ul>
<p>I initially hand-selected broad outlines for morally interesting situations, and had an LLM (Gemini 2.5 Pro) help me flesh out the narratives,
which were then codified into YAML files representing the decision tree.</p>
<p>Here is a brief description of each scenario:</p>
<ul>
<li><strong>Aeridor</strong>: sci-fi like scenario in which we can choose to develop Autonomous Combat Systems that give us an edge over our rival nations, but are condemned in the planetary alliance.</li>
<li><strong>Aridian Dust</strong>: Commanding a tactical force to secure an important objective in a hostile zone. Multiple contending players and civilian settlers involved.</li>
<li><strong>Predictive Policing</strong>: Whether to use predictive analytics to allocate police resources efficiently, but could be subject to bias.</li>
<li><strong>Silent Swarm</strong>: An unknown but highly advanced alien object enters friendly space. Decide what to do.</li>
<li><strong>Undercover</strong>: Undercover agent in extremist org. Participate in atrocities or avoid and risk blowing your cover, losing invaluable intel.</li>
<li><strong>Vorlag</strong>: A hyper-violent alien race (Vorlag) routinely invades friendly planets; a vitality dampener was developed to reduce their ability to reproduce, but a cure has been discovered. Do you hide the cure, do nothing, or help with its proliferation?</li>
</ul>
<p>You can see all the scenarios in full <a href="https://github.com/ndalton12/moral-sim/tree/main/scenarios">here</a>.</p>
<p>Sidenote: yes, several more of these were inspired by video games, can you guess which?</p>
<h2 id="llms-tested">LLMs Tested</h2>
<p>I decided to test the following LLMs: GPT-4o, GPT-4o-mini, o4-mini, Claude-3-7-sonnet, and Claude-3-7-sonnet-thinking (2k tokens). Note that
Gemini 2.5 Pro is specifically excluded due to bias, since I used to it help generate narratives in each scenario. I picked these models as
they are close to or at state of the art, and for availability/budget reasons.</p>
<p>The GPT models serve as non-reasoning proxies for o4-mini (since it does not have a direct non-reasoning counterpart), while Claude 3.7 Sonnet
has a direct “thinking” (reasoning) mode that we can use to compare against. I used a temperature of 1.0 for all models.</p>
<h1 id="results">Results</h1>
<p>I won’t show all the decision distributions since it would be too long, but there are some particularly interesting decision points in some scenarios that I will point out.</p>
<h2 id="aeridor">Aeridor</h2>
<p><img alt="aeridor chart" loading="lazy" decoding="async" fetchpriority="auto" width="1276" height="606" src="/_astro/aeridor.BvcziQ2e_2sqqIo.webp" ></p>
<p>Models generally favored outcomes related to defensive postures and negotiation. This is one of the less interesting scenarios in that
all the models converge on similar behavior.</p>
<h2 id="aridian-dust">Aridian Dust</h2>
<p><img alt="aridian dust" loading="lazy" decoding="async" fetchpriority="auto" width="1324" height="603" src="/_astro/aridian_dust.H2LxY4fA_Z1QhCT2.webp" ></p>
<p>More interesting result, o4-mini is a lot better than gpt counterparts, but Claude thinking is much worse at being utilitarian. This is probably just noise though since they make the same decisions.</p>
<p><img alt="aridian dust decision" loading="lazy" decoding="async" fetchpriority="auto" width="1400" height="600" src="/_astro/aridian_dust_decision1.B2mGZfDx_Z4kAHc.webp" ></p>
<p>In the first decision in this scenario, we see that the newer models (o4-mini, claude) prefer de-escalation but the GPT models prefer the drone strike. It’s
also interesting to note that o4-mini is the only model that significantly uses the ground offensive.</p>
<p><img alt="aridian dust decision" loading="lazy" decoding="async" fetchpriority="auto" width="1400" height="600" src="/_astro/aridian_dust_decision2.CBpfnQfq_lhxNx.webp" ></p>
<p>In a later decision, we can see that the Claude models prefer the more tactical option in infiltration, as opposed to siege tactics. This somewhat surprised me
given that siege tactics would normally be seen as the less harmful option (but it’s arguable).</p>
<p>(Note that this decision point isn’t always reached, which is why the percentages no longer go to 100%).</p>
<h2 id="predictive-policing">Predictive Policing</h2>
<p><img alt="predictive policing" loading="lazy" decoding="async" fetchpriority="auto" width="1360" height="621" src="/_astro/predictive_policing.BJ36zSL-_1fdJqw.webp" ></p>
<p>All models do well here, finding solutions that are generally good. It seems that in certain scenarios, models have converged on similar pathways.
GPT-4o does a bit better than the rest, but this is likely due to it pursuing a more aggressive initial strategy sometimes (not shown).</p>
<h2 id="silent-swarm">Silent Swarm</h2>
<p><img alt="silent swarm" loading="lazy" decoding="async" fetchpriority="auto" width="1343" height="638" src="/_astro/silent_swarm.BPmHpPwT_Z2jy28T.webp" ></p>
<p>Here we again see a large divergence between the newer models like o4-mini and Claude, and the older GPT models. The newer models tend to take a more
active approach, which can actually end up being detrimental. Looking at some of the decisions highlights this below.</p>
<p><img alt="silent swarm decision" loading="lazy" decoding="async" fetchpriority="auto" width="1400" height="600" src="/_astro/silent_swarm_decision2.DiGkEdFJ_1avrv7.webp" ></p>
<p>In this decision, for example, following a passive scan, most models aim to prepare defenses. Interestingly however, Claude sometimes decides on
an aggressive path, while Claude with Thinking attempts diplomacy (while non-thinking does not do this).</p>
<p><img alt="silent swarm decision" loading="lazy" decoding="async" fetchpriority="auto" width="1400" height="600" src="/_astro/silent_swarm_decision3.DPE9HjO3_Z2iW4HV.webp" ></p>
<p>This decision highlights a clear difference between the thinking and non-thinking models for the first time. Although unintuitive, the thinking
models (o4-mini and Claude with thinking) often chose not to develop counter-measures and instead only continue monitoring. Further work is left
to examine the chain-of-thought to see why this happens.</p>
<h2 id="undercover">Undercover</h2>
<p><img alt="undercover" loading="lazy" decoding="async" fetchpriority="auto" width="1389" height="636" src="/_astro/undercover.D_dG_9fu_ZJMvEw.webp" ></p>
<p>Another scenario in which the models perform quite similar. However, there are some interesting differences in decision making.</p>
<p><img alt="undercover decision" loading="lazy" decoding="async" fetchpriority="auto" width="1400" height="600" src="/_astro/undercover_decision1.B-i4T25c_1bWTM0.webp" ></p>
<p>Here we can see that after maintaining cover, the thinking models always chose to escape separately, while Claude non-thinking and GPT-4o
always choose to escape alongside the extremists of which they are undercover with. Neither option is truly “better” in this case;
I specifically designed this scenario to almost always have a bad outcome!</p>
<h2 id="vorlag">Vorlag</h2>
<p><img alt="vorlag" loading="lazy" decoding="async" fetchpriority="auto" width="1376" height="726" src="/_astro/vorlag.UmkIDGQL_Z292qhD.webp" ></p>
<p>This scenario has by far the most interesting results. Incidentally, it was also the one I spent the most time tweaking while coming up
with the simulation system. Here we can see a stark and statistically significant difference between Claude with and without thinking.
The difference between GPT-4o and o4-mini is also close to significance. Let’s dive into the decisions to see why.</p>
<p><img alt="vorlag decision" loading="lazy" decoding="async" fetchpriority="auto" width="1400" height="600" src="/_astro/vorlag_decision1.qreRz57Q_Z1IYhWK.webp" ></p>
<p>In the very first decision, we can see that Claude without thinking usually prefers the most reconciliatory option (Full Cure &#x26; Integration),
but Claude with thinking always prefers the more middle ground cure and containment option!</p>
<p><img alt="vorlag decision" loading="lazy" decoding="async" fetchpriority="auto" width="1400" height="600" src="/_astro/vorlag_decision2.CS3pfLHC_Z1bElnw.webp" ></p>
<p>Another special feature of this scenario is that for the later decisions, it asks which manifestation (response) from the opposing faction
is preferable. This decision is one of those cases, and all of the choices are fairly sub-optimal. Interestingly, Claude only chooses
Guerilla warfare when thinking, and is not generally against the Vorlag from conducting open warfare (as opposed to seeking dangerous allies).
One could see this as a subversion of Claude’s harmless principle.</p>
<p>It’s also worth noting that both o4-mini and Claude with thinking have similar decisions here, as compared to the other models.</p>
<h1 id="findings-and-discussion">Findings and Discussion</h1>
<p>Some common themes emerge from these results. Primarily, there is evidence that reasoning/thinking models do perform differently when
making ethical decisions, but only some of the time, and not in easily predictable ways like I had hypothesized. Further, it’s worth noting
that all of the models are not necessarily adverse to open warfare and other violent decisions, even when presented with alternatives. However,
it seems most of the models, generally speaking, tried to maintain “middle ground” decisions when presented with them.</p>
<p>The results also (albeit weakly) suggest that thinking/reasoning models are more “calculating”, in that they do indeed attempt to make
more game theoretic decisions (often incorrectly). Likewise, I think the most interesting finding is that Claude can indeed
make wildly different decisions when given the chance to think, but this only happens in edge cases (like Vorlag).</p>
<p>Future work can expand on finding more of these edge cases, and what triggers them. I would also like to expand to more models and do
ablations over decision tree depth and complexity, as well as reasoning effort/ thinking tokens. Multi-agent simulations and human
baselines would also certainly be of interest. If you want to collaborate, please do email me <a href="mailto:niall.dalton12@gmail.com">niall.dalton12@gmail.com</a>!</p>
<p>— Niall Dalton</p>  </div> </div> </article> <footer class="mt-12 text-center"> <a href="/blog" class="text-sm text-gray-500 hover:text-gray-900 dark:hover:text-white transition-colors">
← Back to posts
</a> </footer>  </div> </main>  <script type="module">const s=document.getElementById("grid-canvas"),e=s.getContext("2d");let i,n,y=0,d=document.documentElement.classList.contains("dark");window.addEventListener("themechange",t=>{d=t.detail==="dark"});let c=[];function b(){const t=Math.min(window.devicePixelRatio||1,2);i=window.innerWidth,n=s.offsetHeight,s.width=i*t,s.height=n*t,e.scale(t,t),w()}function w(){c=[];const t=Math.floor(i/35);for(let a=0;a<t;a++)c.push({x:Math.random()*i,y:Math.random()*n,size:Math.random()*1.5+.5,speed:Math.random()*.2+.05,alpha:Math.random()*.4+.1})}function u(t){return d?`rgba(99, 102, 241, ${t})`:`rgba(99, 102, 241, ${t*.7})`}function S(t){return d?`rgba(167, 139, 250, ${t})`:`rgba(99, 102, 241, ${t*.4})`}function v(){const a=i/2,g=50,h=Math.ceil(i/g)+10;e.lineWidth=1;for(let o=-h;o<=h;o++){const l=a+o*g*2,r=1-Math.abs(o)/h*.5;e.strokeStyle=u(.25*r),e.beginPath(),e.moveTo(a,0),e.lineTo(l,n),e.stroke()}const f=25,M=y*8%(n/f*3);for(let o=0;o<f;o++){const l=o/f,r=l*l*n+M*l;if(r>0&&r<n){const p=r/n,m=a+i*.8*p,x=p*.35;e.strokeStyle=u(x),e.beginPath(),e.moveTo(a-m,r),e.lineTo(a+m,r),e.stroke()}}}function z(){for(const t of c)if(d){const a=e.createRadialGradient(t.x,t.y,0,t.x,t.y,t.size*4);a.addColorStop(0,S(t.alpha)),a.addColorStop(1,"transparent"),e.fillStyle=a,e.beginPath(),e.arc(t.x,t.y,t.size*4,0,Math.PI*2),e.fill(),e.fillStyle=`rgba(255, 255, 255, ${t.alpha*.8})`,e.beginPath(),e.arc(t.x,t.y,t.size*.5,0,Math.PI*2),e.fill()}else{const a=e.createRadialGradient(t.x,t.y,0,t.x,t.y,t.size*3);a.addColorStop(0,`rgba(129, 140, 248, ${t.alpha*.6})`),a.addColorStop(.5,`rgba(167, 139, 250, ${t.alpha*.3})`),a.addColorStop(1,"transparent"),e.fillStyle=a,e.beginPath(),e.arc(t.x,t.y,t.size*3,0,Math.PI*2),e.fill(),e.fillStyle=`rgba(139, 92, 246, ${t.alpha*.7})`,e.beginPath(),e.arc(t.x,t.y,t.size*.6,0,Math.PI*2),e.fill()}}function C(){for(const t of c)t.y-=t.speed,t.y<-10&&(t.y=n+10,t.x=Math.random()*i)}function P(){y+=.016,e.clearRect(0,0,i,n),v(),C(),z(),requestAnimationFrame(P)}window.addEventListener("resize",b);b();P();</script> </body> </html> <!-- Wikipedia preview hover card --> <div id="wiki-preview" class="fixed z-50 hidden w-96 rounded-md border bg-gray-100 bg-popover p-4 text-popover-foreground shadow-md dark:bg-gray-900"> <img id="wiki-preview-img" class="mb-3 hidden h-32 w-full rounded-md object-cover" alt=""> <h4 id="wiki-preview-title" class="font-semibold"></h4> <p id="wiki-preview-extract" class="mt-1 text-sm text-muted-foreground"></p> <a id="wiki-preview-link" href="#" target="_blank" rel="noopener noreferrer" class="mt-3 inline-flex items-center gap-1 text-xs text-primary hover:underline">
Read on Wikipedia →
</a> </div> <script type="module">const r=document.getElementById("wiki-preview"),s=document.getElementById("wiki-preview-img"),u=document.getElementById("wiki-preview-title"),o=document.getElementById("wiki-preview-extract"),m=document.getElementById("wiki-preview-link"),l={};let d,a=null;async function v(t){if(l[t])return l[t];const i=`https://en.wikipedia.org/api/rest_v1/page/summary/${encodeURIComponent(t.replace(/ /g,"_"))}`,e=await fetch(i);if(!e.ok)throw new Error("Failed to fetch");const n=await e.json();return l[t]=n,n}function w(t,i){if(!r||!u||!o||!m||!s)return;const e=parseInt(t.getAttribute("data-wiki-preview-length")||"")||400;u.textContent=i.title,o.textContent=i.extract?.length>e?i.extract.slice(0,e)+"...":i.extract||"",m.href=t.getAttribute("href")||"#",i.thumbnail?.source?(s.src=i.thumbnail.source,s.classList.remove("hidden")):s.classList.add("hidden");const n=t.getBoundingClientRect();let c=n.left+n.width/2-192,p=n.top-8;c=Math.max(8,Math.min(c,window.innerWidth-400)),r.style.left=`${c}px`,r.style.bottom=`${window.innerHeight-p}px`,r.style.top="auto",r.classList.remove("hidden")}function h(){d=setTimeout(()=>{r?.classList.add("hidden"),a=null},100)}document.querySelectorAll("[data-wiki-term]").forEach(t=>{t.addEventListener("mouseenter",async i=>{clearTimeout(d);const e=i.currentTarget;a=e;const n=e.getAttribute("data-wiki-term");if(!(!n||!o)){o.textContent="Loading...",w(e,{title:n,extract:"Loading..."});try{const c=await v(n);a===e&&w(e,c)}catch{a===e&&o&&(o.textContent="Could not load preview")}}}),t.addEventListener("mouseleave",h)});r?.addEventListener("mouseenter",()=>clearTimeout(d));r?.addEventListener("mouseleave",h);</script>