<!DOCTYPE html><html lang="en" class="dark" data-astro-cid-37fxchfa> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="description" content="What does it mean for our rights to be axiomatic? What about an AI's right?"><meta name="generator" content="Astro v5.16.9"><link rel="icon" type="image/png" href="/icon.png"><link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800&display=swap" rel="stylesheet"><title>Axiomatic Rights, Observational Equivalence, and the Meta-Ethics of Artificial Intelligence | Niall Dalton</title><meta property="og:title" content="Axiomatic Rights, Observational Equivalence, and the Meta-Ethics of Artificial Intelligence | Niall Dalton"><meta property="og:description" content="What does it mean for our rights to be axiomatic? What about an AI's right?"><meta property="og:type" content="website"><meta property="og:url" content="https://ndalton12.github.io/blog/axiomatic-rights/"><!-- Prevent flash of wrong theme --><script>
      const theme = localStorage.getItem('theme') || 'dark';
      if (theme === 'light') {
        document.documentElement.classList.remove('dark');
      }
    </script><link rel="stylesheet" href="/_astro/_slug_.DXTyhdpv.css"></head> <body class="min-h-screen overflow-x-hidden bg-[#f8f9fa] text-gray-900 transition-colors duration-300 dark:bg-[#06060a] dark:text-gray-100" data-astro-cid-37fxchfa> <!-- Background --> <div class="fixed inset-0 -z-10 overflow-hidden" data-astro-cid-37fxchfa> <!-- Dot grid --> <div class="dot-grid" data-astro-cid-37fxchfa></div> <!-- Lower perspective grid --> <canvas id="grid-canvas" data-astro-cid-37fxchfa></canvas> <!-- Animated glow orbs --> <div class="glow glow-1" data-astro-cid-37fxchfa></div> <div class="glow glow-2" data-astro-cid-37fxchfa></div> <!-- Top radial glow --> <div class="absolute inset-0 bg-[radial-gradient(ellipse_60%_40%_at_50%_0%,rgba(99,102,241,0.08))] dark:bg-[radial-gradient(ellipse_60%_40%_at_50%_0%,rgba(120,119,198,0.12))]" data-astro-cid-37fxchfa></div> </div> <main class="relative" data-astro-cid-37fxchfa> <div class="mx-auto max-w-5xl px-6 py-12 sm:px-8 lg:px-12" data-astro-cid-37fxchfa>  <header class="mb-16 pt-8"> <div class="flex flex-col gap-8 md:flex-row md:items-start md:gap-12"> <!-- Profile Image --> <div class="flex-shrink-0"> <a href="/" class="block"> <div class="overflow-hidden rounded-2xl ring-1 ring-gray-200 dark:ring-white/10"> <img class="w-32 h-32 md:w-40 md:h-40 object-cover" src="/me3_compressed.jpeg" alt="Niall Dalton"> </div> </a> </div> <!-- Content --> <div class="flex-1 space-y-4"> <!-- Name with animated gradient --> <div class="flex items-start justify-between gap-4"> <a href="/" class="inline-block"> <h1 class="text-4xl md:text-5xl font-bold tracking-tight"> <span class="gradient-text">Niall Dalton</span> </h1> </a> <button id="theme-toggle" type="button" class="p-2 rounded-lg text-gray-400 hover:text-white hover:bg-white/5 transition-colors" aria-label="Toggle theme"> <!-- Sun icon (shown in dark mode) --> <svg class="w-5 h-5 hidden dark:block" fill="none" stroke="currentColor" viewBox="0 0 24 24"> <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364l-.707-.707M6.343 6.343l-.707-.707m12.728 0l-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z"></path> </svg> <!-- Moon icon (shown in light mode) --> <svg class="w-5 h-5 block dark:hidden" fill="none" stroke="currentColor" viewBox="0 0 24 24"> <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003 0 0012 21a9.003 9.003 0 008.354-5.646z"></path> </svg> </button> <script type="module">const a=document.getElementById("theme-toggle"),t=document.documentElement;function d(){const e=localStorage.getItem("theme");return e==="light"||e==="dark"?e:"dark"}function n(e){e==="dark"?t.classList.add("dark"):t.classList.remove("dark"),localStorage.setItem("theme",e),window.dispatchEvent(new CustomEvent("themechange",{detail:e}))}n(d());a?.addEventListener("click",()=>{const e=t.classList.contains("dark")?"dark":"light";n(e==="dark"?"light":"dark")});</script> </div> <!-- Tagline --> <p class="text-lg text-gray-500 dark:text-gray-400 font-light max-w-md"> I write code and think about stuff. </p> <!-- Navigation Links --> <nav class="pt-2"> <ul class="flex flex-wrap items-center gap-1"> <li> <a href="https://github.com/ndalton12" target="_blank" rel="noopener noreferrer" class="inline-flex items-center gap-2 px-4 py-2 text-sm font-medium text-gray-500 dark:text-gray-400 hover:text-gray-900 dark:hover:text-white rounded-lg transition-colors"> <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24"> <path fill-rule="evenodd" d="M12 2C6.477 2 2 6.484 2 12.017c0 4.425 2.865 8.18 6.839 9.504.5.092.682-.217.682-.483 0-.237-.008-.868-.013-1.703-2.782.605-3.369-1.343-3.369-1.343-.454-1.158-1.11-1.466-1.11-1.466-.908-.62.069-.608.069-.608 1.003.07 1.531 1.032 1.531 1.032.892 1.53 2.341 1.088 2.91.832.092-.647.35-1.088.636-1.338-2.22-.253-4.555-1.113-4.555-4.951 0-1.093.39-1.988 1.029-2.688-.103-.253-.446-1.272.098-2.65 0 0 .84-.27 2.75 1.026A9.564 9.564 0 0112 6.844c.85.004 1.705.115 2.504.337 1.909-1.296 2.747-1.027 2.747-1.027.546 1.379.202 2.398.1 2.651.64.7 1.028 1.595 1.028 2.688 0 3.848-2.339 4.695-4.566 4.943.359.309.678.92.678 1.855 0 1.338-.012 2.419-.012 2.747 0 .268.18.58.688.482A10.019 10.019 0 0022 12.017C22 6.484 17.522 2 12 2z" clip-rule="evenodd"></path> </svg>
GitHub
</a> </li> <li> <a href="https://www.linkedin.com/in/niall-dalton/" target="_blank" rel="noopener noreferrer" class="inline-flex items-center gap-2 px-4 py-2 text-sm font-medium text-gray-500 dark:text-gray-400 hover:text-gray-900 dark:hover:text-white rounded-lg transition-colors"> <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24"> <path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"></path> </svg>
LinkedIn
</a> </li> <li> <a href="/blog" class="inline-flex items-center gap-2 px-4 py-2 text-sm font-medium text-gray-500 dark:text-gray-400 hover:text-gray-900 dark:hover:text-white rounded-lg transition-colors"> <svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"> <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 20H5a2 2 0 01-2-2V6a2 2 0 012-2h10a2 2 0 012 2v1m2 13a2 2 0 01-2-2V7m2 13a2 2 0 002-2V9a2 2 0 00-2-2h-2m-4-3H9M7 16h6M7 8h6v4H7V8z"></path> </svg>
Blog
</a> </li> </ul> </nav> </div> </div> </header> <article> <header class="mb-12"> <div class="glass-card rounded-2xl p-6 md:p-8"> <a href="/blog" class="text-sm text-gray-500 hover:text-gray-900 dark:hover:text-white transition-colors">
← Back
</a> <h1 class="mt-6 text-3xl md:text-4xl font-bold text-gray-900 dark:text-white tracking-tight"> Axiomatic Rights, Observational Equivalence, and the Meta-Ethics of Artificial Intelligence </h1> <time class="mt-3 block text-sm text-gray-500"> November 5, 2023 </time> </div> </header> <div class="glass-card rounded-2xl p-6 md:p-8"> <div class="prose max-w-none">  <p>What does it mean for our rights to be axiomatic? Should an AI have axiomatic rights?</p>
<p>A central question in philosophy is deciding how to act ethically. While popular theories of normative ethics include <a href="https://en.wikipedia.org/wiki/Virtue_ethics" target="_blank" rel="noopener noreferrer" class="text-primary underline decoration-dotted hover:decoration-solid" data-wiki-term="Virtue ethics">virtue ethics</a> and <a href="https://en.wikipedia.org/wiki/Consequentialism" target="_blank" rel="noopener noreferrer" class="text-primary underline decoration-dotted hover:decoration-solid" data-wiki-term="Consequentialism">consequentialism</a>, another idea is that of <a href="https://en.wikipedia.org/wiki/Pragmatic_ethics" target="_blank" rel="noopener noreferrer" class="text-primary underline decoration-dotted hover:decoration-solid" data-wiki-term="Pragmatic ethics">pragmatic ethics</a> — loosely speaking, that is the theory in which the ‘correct’ ethical theory is an ever-evolving set of propositions which may change over time due to new understanding.</p>
<p>One problem with such a theory is that there is no central body for ethics, no over-arching governing set of leading hypotheses which can be subjected to rigorous experiment and then peer review. The closest thing we have as humans today is probably the <a href="https://en.wikipedia.org/wiki/Universal_Declaration_of_Human_Rights" target="_blank" rel="noopener noreferrer" class="text-primary underline decoration-dotted hover:decoration-solid" data-wiki-term="Universal Declaration of Human Rights">Universal Declaration of Human Rights (UDHR)</a>.</p>
<p>And indeed, we do see that human rights shift over time. Take for example the <a href="https://en.wikipedia.org/wiki/Suffragette" target="_blank" rel="noopener noreferrer" class="text-primary underline decoration-dotted hover:decoration-solid" data-wiki-term="Suffragette">Suffragette</a> movement, prior to which women did not have the right to vote in many countries. However, we have little reason to believe that the current state of ‘generally accepted ethics’ (say, the UDHR) will — or should — remain unchanged.</p>
<p>Looking beyond the horizon, the rise of artificial intelligences with the ability to communicate in a human-like manner will create the need for us as a society to grapple with the idea of AI ethics. Given that our current systems are generally not sufficient for reasoning about such ethics, consider the idea of grounding a pragmatic ethical system in axiomatic rights.</p>
<p>An <a href="https://en.wikipedia.org/wiki/Axiom" target="_blank" rel="noopener noreferrer" class="text-primary underline decoration-dotted hover:decoration-solid" data-wiki-term="Axiom">axiom</a> is a self-evident truth that is used as a basis for further reasoning. One possible axiomatic basis for rights would be to use the UHDR’s elements as axioms. In other words, take human rights as the basic, unassailable principles on which to further reason about ethics. Such a basis circumvents many problems with theories such as <a href="https://en.wikipedia.org/wiki/Utilitarianism" target="_blank" rel="noopener noreferrer" class="text-primary underline decoration-dotted hover:decoration-solid" data-wiki-term="Utilitarianism">utilitarianism</a>, in which murder may be justified if a person produces little to no societal value — under axiomatic rights, a person always has a right to live.</p>
<p>But how does this relate to AI ethics? An AI is not a human. Well, under both axiomatic rights and pragmatic ethics, our definition of what a person is — or more broadly, ‘what’ gets axiomatic rights similar to the UDHR — could change. In an allusion to the famous <a href="https://en.wikipedia.org/wiki/Turing_test" target="_blank" rel="noopener noreferrer" class="text-primary underline decoration-dotted hover:decoration-solid" data-wiki-term="Turing test">Turing test</a>, consider an AI that has a body and mind that are indistinguishable from a human. In many ways, this AI could be considered conscious, and have subjective experience (and be self-aware of those experiences and reason about them).</p>
<p>Put another way, it may have “symptoms” of consciousness such that, to a rational observer, the AI is <a href="https://en.wikipedia.org/wiki/Observational_equivalence" target="_blank" rel="noopener noreferrer" class="text-primary underline decoration-dotted hover:decoration-solid" data-wiki-term="Observational equivalence">observationally equivalent</a> to a human. One could then imagine that this “symptomatically conscious” AI is, in some way, equally deserving of the same axiomatic rights as a human.</p>
<p>Although today’s AIs are not observationally equivalent to humans in most ways, they are already equivalent in a few. For example, today’s chatbots <a href="https://arxiv.org/pdf/2303.12712.pdf">can answer challenging test questions</a> to a surprisingly human level. In this sense, these chatbots are observationally equivalent to an abstract human test taker. Often times graders will only receive the output of a human — for example, the marks a person wrote down on a test paper. This problem is already an issue for many teachers, who struggle to distinguish between legitimately written essays and those written by a chatbot.</p>
<p>Thus, we must consider the idea that future AIs will have higher levels of observational equivalence to humans, and possibly be deserving of more rights. However, with rights comes responsibilities. In the same framework as pragmatic ethics, we as society have certain expectations of individuals — that they will act reasonably and in accordance with law. In general, we also have higher expectations of those who are older (but, interestingly, not necessarily those who are smarter). Likewise, we should have expectations that a more powerful an AI is, the more we should expect it to behave in accordance with ethical systems built on top of axiomatic rights.</p>
<p>Still, we <em>must</em> ensure that the systems in which we surround our AI with are widely agreed upon, and strong enough to prevent the use of AI in the proliferation of injustice, bias, and systematic oppression. And to an unsettlingly large extent, AI is already causing <a href="https://www.scientificamerican.com/article/humans-absorb-bias-from-ai-and-keep-it-after-they-stop-using-the-algorithm/">such problems</a>. As not only the creators, but the judge, jury, and executioners of AI, we have a responsibility, too, to ensure that AI elevates the common good (or otherwise behaves ethically and lawfully).</p>
<p>Developing a principled system in which to enact AI ethics is certainly a new and challenging problem, but one which presents us with the opportunity to change our theories, minds, and behaviors in ways that could greatly improve the human (and machine?) condition for centuries to come.</p>
<p>— ND</p>  </div> </div> </article> <footer class="mt-12 text-center"> <a href="/blog" class="text-sm text-gray-500 hover:text-gray-900 dark:hover:text-white transition-colors">
← Back to posts
</a> </footer>  </div> </main>  <script type="module">const s=document.getElementById("grid-canvas"),e=s.getContext("2d");let i,n,y=0,d=document.documentElement.classList.contains("dark");window.addEventListener("themechange",t=>{d=t.detail==="dark"});let c=[];function b(){const t=Math.min(window.devicePixelRatio||1,2);i=window.innerWidth,n=s.offsetHeight,s.width=i*t,s.height=n*t,e.scale(t,t),w()}function w(){c=[];const t=Math.floor(i/35);for(let a=0;a<t;a++)c.push({x:Math.random()*i,y:Math.random()*n,size:Math.random()*1.5+.5,speed:Math.random()*.2+.05,alpha:Math.random()*.4+.1})}function u(t){return d?`rgba(99, 102, 241, ${t})`:`rgba(99, 102, 241, ${t*.7})`}function S(t){return d?`rgba(167, 139, 250, ${t})`:`rgba(99, 102, 241, ${t*.4})`}function v(){const a=i/2,g=50,h=Math.ceil(i/g)+10;e.lineWidth=1;for(let o=-h;o<=h;o++){const l=a+o*g*2,r=1-Math.abs(o)/h*.5;e.strokeStyle=u(.25*r),e.beginPath(),e.moveTo(a,0),e.lineTo(l,n),e.stroke()}const f=25,M=y*8%(n/f*3);for(let o=0;o<f;o++){const l=o/f,r=l*l*n+M*l;if(r>0&&r<n){const p=r/n,m=a+i*.8*p,x=p*.35;e.strokeStyle=u(x),e.beginPath(),e.moveTo(a-m,r),e.lineTo(a+m,r),e.stroke()}}}function z(){for(const t of c)if(d){const a=e.createRadialGradient(t.x,t.y,0,t.x,t.y,t.size*4);a.addColorStop(0,S(t.alpha)),a.addColorStop(1,"transparent"),e.fillStyle=a,e.beginPath(),e.arc(t.x,t.y,t.size*4,0,Math.PI*2),e.fill(),e.fillStyle=`rgba(255, 255, 255, ${t.alpha*.8})`,e.beginPath(),e.arc(t.x,t.y,t.size*.5,0,Math.PI*2),e.fill()}else{const a=e.createRadialGradient(t.x,t.y,0,t.x,t.y,t.size*3);a.addColorStop(0,`rgba(129, 140, 248, ${t.alpha*.6})`),a.addColorStop(.5,`rgba(167, 139, 250, ${t.alpha*.3})`),a.addColorStop(1,"transparent"),e.fillStyle=a,e.beginPath(),e.arc(t.x,t.y,t.size*3,0,Math.PI*2),e.fill(),e.fillStyle=`rgba(139, 92, 246, ${t.alpha*.7})`,e.beginPath(),e.arc(t.x,t.y,t.size*.6,0,Math.PI*2),e.fill()}}function C(){for(const t of c)t.y-=t.speed,t.y<-10&&(t.y=n+10,t.x=Math.random()*i)}function P(){y+=.016,e.clearRect(0,0,i,n),v(),C(),z(),requestAnimationFrame(P)}window.addEventListener("resize",b);b();P();</script> </body> </html> <!-- Wikipedia preview hover card --> <div id="wiki-preview" class="fixed z-50 hidden w-96 rounded-md border bg-gray-100 bg-popover p-4 text-popover-foreground shadow-md dark:bg-gray-900"> <img id="wiki-preview-img" class="mb-3 hidden h-32 w-full rounded-md object-cover" alt=""> <h4 id="wiki-preview-title" class="font-semibold"></h4> <p id="wiki-preview-extract" class="mt-1 text-sm text-muted-foreground"></p> <a id="wiki-preview-link" href="#" target="_blank" rel="noopener noreferrer" class="mt-3 inline-flex items-center gap-1 text-xs text-primary hover:underline">
Read on Wikipedia →
</a> </div> <script type="module">const r=document.getElementById("wiki-preview"),s=document.getElementById("wiki-preview-img"),u=document.getElementById("wiki-preview-title"),o=document.getElementById("wiki-preview-extract"),m=document.getElementById("wiki-preview-link"),l={};let d,a=null;async function v(t){if(l[t])return l[t];const i=`https://en.wikipedia.org/api/rest_v1/page/summary/${encodeURIComponent(t.replace(/ /g,"_"))}`,e=await fetch(i);if(!e.ok)throw new Error("Failed to fetch");const n=await e.json();return l[t]=n,n}function w(t,i){if(!r||!u||!o||!m||!s)return;const e=parseInt(t.getAttribute("data-wiki-preview-length")||"")||400;u.textContent=i.title,o.textContent=i.extract?.length>e?i.extract.slice(0,e)+"...":i.extract||"",m.href=t.getAttribute("href")||"#",i.thumbnail?.source?(s.src=i.thumbnail.source,s.classList.remove("hidden")):s.classList.add("hidden");const n=t.getBoundingClientRect();let c=n.left+n.width/2-192,p=n.top-8;c=Math.max(8,Math.min(c,window.innerWidth-400)),r.style.left=`${c}px`,r.style.bottom=`${window.innerHeight-p}px`,r.style.top="auto",r.classList.remove("hidden")}function h(){d=setTimeout(()=>{r?.classList.add("hidden"),a=null},100)}document.querySelectorAll("[data-wiki-term]").forEach(t=>{t.addEventListener("mouseenter",async i=>{clearTimeout(d);const e=i.currentTarget;a=e;const n=e.getAttribute("data-wiki-term");if(!(!n||!o)){o.textContent="Loading...",w(e,{title:n,extract:"Loading..."});try{const c=await v(n);a===e&&w(e,c)}catch{a===e&&o&&(o.textContent="Could not load preview")}}}),t.addEventListener("mouseleave",h)});r?.addEventListener("mouseenter",()=>clearTimeout(d));r?.addEventListener("mouseleave",h);</script>