<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="description" content="What does it mean for our rights to be axiomatic? What about an AI's right?"><meta name="generator" content="Astro v5.16.9"><link rel="icon" type="image/png" href="/icon.png"><title>Axiomatic Rights, Observational Equivalence, and the Meta-Ethics of Artificial Intelligence | Niall Dalton</title><!-- Open Graph --><meta property="og:title" content="Axiomatic Rights, Observational Equivalence, and the Meta-Ethics of Artificial Intelligence | Niall Dalton"><meta property="og:description" content="What does it mean for our rights to be axiomatic? What about an AI's right?"><meta property="og:type" content="website"><meta property="og:url" content="https://ndalton12.github.io/blog/axiomatic-rights/"><link rel="stylesheet" href="/_astro/_slug_.GIvBMdMb.css"></head> <body> <div class="relative"> <!-- Decorative dots pattern --> <svg class="hidden fixed transform right-0 top-[5%] z-0 xl:block" width="404" height="784" fill="none" viewBox="0 0 404 784"> <defs> <pattern id="dots-pattern" x="0" y="0" width="20" height="20" patternUnits="userSpaceOnUse"> <rect x="0" y="0" width="4" height="4" class="text-gray-200" fill="#edf2f7"></rect> </pattern> </defs> <rect width="404" height="784" fill="url(#dots-pattern)"></rect> </svg> <main class="p-8 relative lg:max-w-[75%] max-w-prose xs:p-24">  <header class="block mb-6 md:flex"> <div class="w-full max-w-[150px]"> <a href="/"> <img class="rounded-full transform transition-all duration-150 hover:scale-105" src="/me3_compressed.jpeg" alt="Niall Dalton"> </a> </div> <div class="flex-none pt-6 md:pt-1 md:flex-1 md:pl-20"> <h1 class="text-5xl text-primary font-bold leading-tight hover:text-primary/80 animate-fade-in-down"> <a href="/">Niall Dalton</a> </h1> <p class="text-gray-600">I write code and think about stuff.</p> <ul class="mt-6 uppercase tracking-wider"> <li class="inline list-none pr-4"> <a class="inline-block py-2 font-semibold text-xs text-gray-600 hover:text-black" href="https://github.com/ndalton12" target="_blank" rel="noopener noreferrer">
GitHub
</a> </li> <li class="inline list-none pr-4"> <a class="inline-block py-2 font-semibold text-xs text-gray-600 hover:text-black" href="https://www.linkedin.com/in/niall-dalton/" target="_blank" rel="noopener noreferrer">
LinkedIn
</a> </li> <li class="inline list-none pr-4"> <a class="inline-block py-2 font-semibold text-xs text-gray-600 hover:text-black" href="/blog">
Blog
</a> </li> </ul> </div> </header> <article> <h1 class="mt-16 text-4xl text-gray-900 font-bold">Axiomatic Rights, Observational Equivalence, and the Meta-Ethics of Artificial Intelligence</h1> <p class="text-gray-600 font-light">Posted on November 5, 2023</p> <div class="mt-16 prose">  <p>What does it mean for our rights to be axiomatic? Should an AI have axiomatic rights?</p>
<p>A central question in philosophy is deciding how to act ethically. While popular theories of normative ethics include <a href="https://en.wikipedia.org/wiki/Virtue_ethics" target="_blank" rel="noopener noreferrer" class="text-primary underline decoration-dotted hover:decoration-solid" data-wiki-term="Virtue ethics">virtue ethics</a> and <a href="https://en.wikipedia.org/wiki/Consequentialism" target="_blank" rel="noopener noreferrer" class="text-primary underline decoration-dotted hover:decoration-solid" data-wiki-term="Consequentialism">consequentialism</a>, another idea is that of <a href="https://en.wikipedia.org/wiki/Pragmatic_ethics" target="_blank" rel="noopener noreferrer" class="text-primary underline decoration-dotted hover:decoration-solid" data-wiki-term="Pragmatic ethics">pragmatic ethics</a> — loosely speaking, that is the theory in which the ‘correct’ ethical theory is an ever-evolving set of propositions which may change over time due to new understanding.</p>
<p>One problem with such a theory is that there is no central body for ethics, no over-arching governing set of leading hypotheses which can be subjected to rigorous experiment and then peer review. The closest thing we have as humans today is probably the <a href="https://en.wikipedia.org/wiki/Universal_Declaration_of_Human_Rights" target="_blank" rel="noopener noreferrer" class="text-primary underline decoration-dotted hover:decoration-solid" data-wiki-term="Universal Declaration of Human Rights">Universal Declaration of Human Rights (UDHR)</a>.</p>
<p>And indeed, we do see that human rights shift over time. Take for example the <a href="https://en.wikipedia.org/wiki/Suffragette" target="_blank" rel="noopener noreferrer" class="text-primary underline decoration-dotted hover:decoration-solid" data-wiki-term="Suffragette">Suffragette</a> movement, prior to which women did not have the right to vote in many countries. However, we have little reason to believe that the current state of ‘generally accepted ethics’ (say, the UDHR) will — or should — remain unchanged.</p>
<p>Looking beyond the horizon, the rise of artificial intelligences with the ability to communicate in a human-like manner will create the need for us as a society to grapple with the idea of AI ethics. Given that our current systems are generally not sufficient for reasoning about such ethics, consider the idea of grounding a pragmatic ethical system in axiomatic rights.</p>
<p>An <a href="https://en.wikipedia.org/wiki/Axiom" target="_blank" rel="noopener noreferrer" class="text-primary underline decoration-dotted hover:decoration-solid" data-wiki-term="Axiom">axiom</a> is a self-evident truth that is used as a basis for further reasoning. One possible axiomatic basis for rights would be to use the UHDR’s elements as axioms. In other words, take human rights as the basic, unassailable principles on which to further reason about ethics. Such a basis circumvents many problems with theories such as <a href="https://en.wikipedia.org/wiki/Utilitarianism" target="_blank" rel="noopener noreferrer" class="text-primary underline decoration-dotted hover:decoration-solid" data-wiki-term="Utilitarianism">utilitarianism</a>, in which murder may be justified if a person produces little to no societal value — under axiomatic rights, a person always has a right to live.</p>
<p>But how does this relate to AI ethics? An AI is not a human. Well, under both axiomatic rights and pragmatic ethics, our definition of what a person is — or more broadly, ‘what’ gets axiomatic rights similar to the UDHR — could change. In an allusion to the famous <a href="https://en.wikipedia.org/wiki/Turing_test" target="_blank" rel="noopener noreferrer" class="text-primary underline decoration-dotted hover:decoration-solid" data-wiki-term="Turing test">Turing test</a>, consider an AI that has a body and mind that are indistinguishable from a human. In many ways, this AI could be considered conscious, and have subjective experience (and be self-aware of those experiences and reason about them).</p>
<p>Put another way, it may have “symptoms” of consciousness such that, to a rational observer, the AI is <a href="https://en.wikipedia.org/wiki/Observational_equivalence" target="_blank" rel="noopener noreferrer" class="text-primary underline decoration-dotted hover:decoration-solid" data-wiki-term="Observational equivalence">observationally equivalent</a> to a human. One could then imagine that this “symptomatically conscious” AI is, in some way, equally deserving of the same axiomatic rights as a human.</p>
<p>Although today’s AIs are not observationally equivalent to humans in most ways, they are already equivalent in a few. For example, today’s chatbots <a href="https://arxiv.org/pdf/2303.12712.pdf">can answer challenging test questions</a> to a surprisingly human level. In this sense, these chatbots are observationally equivalent to an abstract human test taker. Often times graders will only receive the output of a human — for example, the marks a person wrote down on a test paper. This problem is already an issue for many teachers, who struggle to distinguish between legitimately written essays and those written by a chatbot.</p>
<p>Thus, we must consider the idea that future AIs will have higher levels of observational equivalence to humans, and possibly be deserving of more rights. However, with rights comes responsibilities. In the same framework as pragmatic ethics, we as society have certain expectations of individuals — that they will act reasonably and in accordance with law. In general, we also have higher expectations of those who are older (but, interestingly, not necessarily those who are smarter). Likewise, we should have expectations that a more powerful an AI is, the more we should expect it to behave in accordance with ethical systems built on top of axiomatic rights.</p>
<p>Still, we <em>must</em> ensure that the systems in which we surround our AI with are widely agreed upon, and strong enough to prevent the use of AI in the proliferation of injustice, bias, and systematic oppression. And to an unsettlingly large extent, AI is already causing <a href="https://www.scientificamerican.com/article/humans-absorb-bias-from-ai-and-keep-it-after-they-stop-using-the-algorithm/">such problems</a>. As not only the creators, but the judge, jury, and executioners of AI, we have a responsibility, too, to ensure that AI elevates the common good (or otherwise behaves ethically and lawfully).</p>
<p>Developing a principled system in which to enact AI ethics is certainly a new and challenging problem, but one which presents us with the opportunity to change our theories, minds, and behaviors in ways that could greatly improve the human (and machine?) condition for centuries to come.</p>
<p>— ND</p>  </div> </article>  </main> </div> </body></html> <!-- Wikipedia preview hover card --> <div id="wiki-preview" class="fixed z-50 hidden w-96 rounded-md border bg-popover p-4 text-popover-foreground shadow-md"> <img id="wiki-preview-img" class="mb-3 hidden h-32 w-full rounded-md object-cover" alt=""> <h4 id="wiki-preview-title" class="font-semibold"></h4> <p id="wiki-preview-extract" class="mt-1 text-sm text-muted-foreground"></p> <a id="wiki-preview-link" href="#" target="_blank" rel="noopener noreferrer" class="mt-3 inline-flex items-center gap-1 text-xs text-primary hover:underline">
Read on Wikipedia →
</a> </div> <script type="module">const r=document.getElementById("wiki-preview"),s=document.getElementById("wiki-preview-img"),u=document.getElementById("wiki-preview-title"),o=document.getElementById("wiki-preview-extract"),m=document.getElementById("wiki-preview-link"),l={};let d,a=null;async function v(t){if(l[t])return l[t];const i=`https://en.wikipedia.org/api/rest_v1/page/summary/${encodeURIComponent(t.replace(/ /g,"_"))}`,e=await fetch(i);if(!e.ok)throw new Error("Failed to fetch");const n=await e.json();return l[t]=n,n}function w(t,i){if(!r||!u||!o||!m||!s)return;const e=parseInt(t.getAttribute("data-wiki-preview-length")||"")||400;u.textContent=i.title,o.textContent=i.extract?.length>e?i.extract.slice(0,e)+"...":i.extract||"",m.href=t.getAttribute("href")||"#",i.thumbnail?.source?(s.src=i.thumbnail.source,s.classList.remove("hidden")):s.classList.add("hidden");const n=t.getBoundingClientRect();let c=n.left+n.width/2-192,p=n.top-8;c=Math.max(8,Math.min(c,window.innerWidth-400)),r.style.left=`${c}px`,r.style.bottom=`${window.innerHeight-p}px`,r.style.top="auto",r.classList.remove("hidden")}function h(){d=setTimeout(()=>{r?.classList.add("hidden"),a=null},100)}document.querySelectorAll("[data-wiki-term]").forEach(t=>{t.addEventListener("mouseenter",async i=>{clearTimeout(d);const e=i.currentTarget;a=e;const n=e.getAttribute("data-wiki-term");if(!(!n||!o)){o.textContent="Loading...",w(e,{title:n,extract:"Loading..."});try{const c=await v(n);a===e&&w(e,c)}catch{a===e&&o&&(o.textContent="Could not load preview")}}}),t.addEventListener("mouseleave",h)});r?.addEventListener("mouseenter",()=>clearTimeout(d));r?.addEventListener("mouseleave",h);</script>